{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #Trabalhar com matrizes\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.manifold import TSNE\n",
    "import re #Regex\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download de recursos NLTK (se necessário)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto 1: Este filme é incrível, adorei a atuação do protagonista\n",
      "Texto 2: A direção de fotografia é espetacular e o roteiro é envolvente\n",
      "Texto 3: Péssimo filme, desperdicei meu tempo assistindo isso\n"
     ]
    }
   ],
   "source": [
    "# Dados de exemplo - críticas de filmes (simplificadas)\n",
    "textos = [\n",
    "    \"Este filme é incrível, adorei a atuação do protagonista\",\n",
    "    \"A direção de fotografia é espetacular e o roteiro é envolvente\",\n",
    "    \"Péssimo filme, desperdicei meu tempo assistindo isso\",\n",
    "    \"Os atores são talentosos mas o roteiro é fraco\",\n",
    "    \"Cinematografia belíssima, recomendo assistir no cinema\",\n",
    "    \"Não gostei da história, personagens mal desenvolvidos\",\n",
    "    \"A trilha sonora combina perfeitamente com as cenas\",\n",
    "    \"Filme entediante, previsível do início ao fim\",\n",
    "    \"Os efeitos especiais são impressionantes, tecnologia de ponta\",\n",
    "    \"História emocionante, chorei no final do filme\"\n",
    "]\n",
    "\n",
    "# Verificando os dados\n",
    "for i, texto in enumerate(textos[:3]):  # Mostrando apenas os 3 primeiros\n",
    "    print(f\"Texto {i+1}: {texto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo de texto original:\n",
      "Este filme é incrível, adorei a atuação do protagonista\n",
      "\n",
      "Depois do pré-processamento:\n",
      "['filme', 'incrível', 'adorei', 'atuação', 'protagonista']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocessar_texto(texto):\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Remover caracteres especiais e números\n",
    "    texto = re.sub(r'[^a-záàâãéèêíïóôõöúçñ ]', '', texto)\n",
    "\n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(texto)\n",
    "\n",
    "    # Remover stopwords (opcional, dependendo da aplicação)\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Aplicar pré-processamento a todos os textos\n",
    "textos_preprocessados = [preprocessar_texto(texto) for texto in textos]\n",
    "\n",
    "# Verificar resultado\n",
    "print(\"Exemplo de texto original:\")\n",
    "print(textos[0])\n",
    "print(\"\\nDepois do pré-processamento:\")\n",
    "print(textos_preprocessados[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parâmetros do modelo\n",
    "vector_size = 100    # Dimensionalidade dos vetores\n",
    "window = 5           # Tamanho da janela de contexto\n",
    "min_count = 1        # Frequência mínima das palavras\n",
    "workers = 4          # Número de threads para treinamento\n",
    "sg = 1               # Modelo Skip-gram (1) ou CBOW (0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "model = Word2Vec(\n",
    "    sentences=textos_preprocessados,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    sg=sg\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado com 44 palavras no vocabulário\n"
     ]
    }
   ],
   "source": [
    "print(f\"Modelo treinado com {len(model.wv.key_to_index)} palavras no vocabulário\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algumas palavras do vocabulário:\n",
      "['filme', 'história', 'roteiro', 'desperdicei', 'belíssima', 'cinematografia', 'fraco', 'talentosos', 'atores', 'assistindo']\n"
     ]
    }
   ],
   "source": [
    "# Listar algumas palavras do vocabulário\n",
    "palavras = list(model.wv.key_to_index.keys())\n",
    "print(\"Algumas palavras do vocabulário:\")\n",
    "print(palavras[:10])  # Primeiras 10 palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['filme', 'história', 'roteiro', 'desperdicei', 'belíssima', 'cinematografia', 'fraco', 'talentosos', 'atores', 'assistindo', 'tempo', 'péssimo', 'assistir', 'envolvente', 'espetacular', 'fotografia', 'direção', 'protagonista', 'atuação', 'adorei', 'incrível', 'recomendo', 'final', 'chorei', 'previsível', 'emocionante', 'ponta', 'tecnologia', 'impressionantes', 'especiais', 'efeitos', 'fim', 'início', 'entediante', 'gostei', 'cenas', 'perfeitamente', 'combina', 'sonora', 'trilha', 'desenvolvidos', 'mal', 'personagens', 'cinema']\n"
     ]
    }
   ],
   "source": [
    "if 'filme' in model.wv:\n",
    "    vector_size = palavras\n",
    "    print(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palavras mais similares a 'filme':\n",
      "direção: 0.2189\n",
      "assistindo: 0.2168\n",
      "impressionantes: 0.1955\n",
      "cinema: 0.1694\n",
      "efeitos: 0.1520\n"
     ]
    }
   ],
   "source": [
    "# Encontrar palavras mais similares a 'filme'\n",
    "if 'filme' in model.wv:\n",
    "    similares = model.wv.most_similar('filme', topn=5)\n",
    "    print(\"\\nPalavras mais similares a 'filme':\")\n",
    "    for palavra, similaridade in similares:\n",
    "        print(f\"{palavra}: {similaridade:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('vasco.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alguns modelos disponíveis no gensim-data:\n",
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50']\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "Modelo pré-treinado carregado com 400000 palavras\n",
      "\n",
      "Palavras mais similares a 'computer':\n",
      "computers: 0.8752\n",
      "software: 0.8373\n",
      "technology: 0.7642\n",
      "pc: 0.7366\n",
      "hardware: 0.7290\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Listar modelos disponíveis\n",
    "print(\"Alguns modelos disponíveis no gensim-data:\")\n",
    "print([nome for nome in list(api.info()['models'].keys())[:10]])\n",
    "\n",
    "# Carregar um modelo pré-treinado (word2vec-google-news-300)\n",
    "# Nota: Isso pode demorar um pouco na primeira execução (download)\n",
    "try:\n",
    "    # Para português, você pode tentar 'word2vec-portuguese' se disponível\n",
    "    # ou usar modelos do NILC: http://nilc.icmc.usp.br/embeddings\n",
    "    modelo_pretreino = api.load(\"glove-wiki-gigaword-100\")  # Mais rápido que word2vec-google-news-300\n",
    "    print(f\"Modelo pré-treinado carregado com {len(modelo_pretreino.key_to_index)} palavras\")\n",
    "\n",
    "    # Verificar palavras similares usando modelo pré-treinado\n",
    "    if 'computer' in modelo_pretreino:\n",
    "        similares = modelo_pretreino.most_similar('computer', topn=5)\n",
    "        print(\"\\nPalavras mais similares a 'computer':\")\n",
    "        for palavra, similaridade in similares:\n",
    "            print(f\"{palavra}: {similaridade:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar modelo pré-treinado: {e}\")\n",
    "    print(\"Tente outro modelo ou prossiga sem esta parte\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analogia: rei - homem + mulher =\n",
      "queen: 0.7699\n",
      "monarch: 0.6843\n",
      "throne: 0.6756\n"
     ]
    }
   ],
   "source": [
    "# Usando modelo pré-treinado para analogias (se disponível)\n",
    "try:\n",
    "    # Famosa analogia: rei - homem + mulher = rainha\n",
    "    if all(word in modelo_pretreino for word in ['king', 'man', 'woman']):\n",
    "        resultado = modelo_pretreino.most_similar(\n",
    "            positive=['king', 'woman'],\n",
    "            negative=['man'],\n",
    "            topn=3\n",
    "        )\n",
    "        print(\"\\nAnalogia: rei - homem + mulher =\")\n",
    "        for palavra, similaridade in resultado:\n",
    "            print(f\"{palavra}: {similaridade:.4f}\")\n",
    "except:\n",
    "    print(\"Não foi possível realizar a operação de analogia com o modelo disponível\")\n",
    "\n",
    "# Podemos tentar outras analogias com nosso modelo treinado\n",
    "# Por exemplo: bom - positivo + negativo = ruim\n",
    "# (Dependendo do tamanho do corpus, pode não funcionar bem para modelos pequenos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alguns modelos disponíveis no gensim-data:\n",
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50']\n",
      "Modelo pré-treinado carregado com 400000 palavras\n",
      "\n",
      "Palavras mais similares a 'computer':\n",
      "computers: 0.8752\n",
      "software: 0.8373\n",
      "technology: 0.7642\n",
      "pc: 0.7366\n",
      "hardware: 0.7290\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Listar modelos disponíveis\n",
    "print(\"Alguns modelos disponíveis no gensim-data:\")\n",
    "print([nome for nome in list(api.info()['models'].keys())[:10]])\n",
    "\n",
    "# Carregar um modelo pré-treinado (word2vec-google-news-300)\n",
    "# Nota: Isso pode demorar um pouco na primeira execução (download)\n",
    "try:\n",
    "    # Para português, você pode tentar 'word2vec-portuguese' se disponível\n",
    "    # ou usar modelos do NILC: http://nilc.icmc.usp.br/embeddings\n",
    "    modelo_pretreino = api.load(\"glove-wiki-gigaword-100\")  # Mais rápido que word2vec-google-news-300\n",
    "    print(f\"Modelo pré-treinado carregado com {len(modelo_pretreino.key_to_index)} palavras\")\n",
    "\n",
    "    # Verificar palavras similares usando modelo pré-treinado\n",
    "    if 'computer' in modelo_pretreino:\n",
    "        similares = modelo_pretreino.most_similar('computer', topn=5)\n",
    "        print(\"\\nPalavras mais similares a 'computer':\")\n",
    "        for palavra, similaridade in similares:\n",
    "            print(f\"{palavra}: {similaridade:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar modelo pré-treinado: {e}\")\n",
    "    print(\"Tente outro modelo ou prossiga sem esta parte\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular similaridade entre pares de palavras\n",
    "def calcular_similaridade(modelo, pares_palavras):\n",
    "    resultados = []\n",
    "    for par in pares_palavras:\n",
    "        palavra1, palavra2 = par\n",
    "        if palavra1 in modelo.wv and palavra2 in modelo.wv:\n",
    "            similaridade = modelo.wv.similarity(palavra1, palavra2)\n",
    "            resultados.append((par, similaridade))\n",
    "        else:\n",
    "            resultados.append((par, \"Uma ou ambas as palavras não estão no vocabulário\"))\n",
    "    return resultados\n",
    "\n",
    "# Pares de palavras para testar\n",
    "pares = [\n",
    "    ('filme', 'cinema'),\n",
    "    ('bom', 'ruim'),\n",
    "    ('ator', 'atuação'),\n",
    "    ('filme', 'protagonista')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similaridade entre pares de palavras:\n",
      "filme - cinema: 0.1693534404039383\n",
      "bom - ruim: Uma ou ambas as palavras não estão no vocabulário\n",
      "ator - atuação: Uma ou ambas as palavras não estão no vocabulário\n",
      "filme - protagonista: -0.11359719187021255\n"
     ]
    }
   ],
   "source": [
    "# Calcular similaridades\n",
    "similaridades = calcular_similaridade(model, pares)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"\\nSimilaridade entre pares de palavras:\")\n",
    "for (palavra1, palavra2), similaridade in similaridades:\n",
    "    if isinstance(similaridade, float):\n",
    "        print(f\"{palavra1} - {palavra2}: {similaridade:.4f}\")\n",
    "    else:\n",
    "        print(f\"{palavra1} - {palavra2}: {similaridade}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_embeddings(modelo, palavras=None, n_palavras=50):\n",
    "    \"\"\"\n",
    "    Visualiza os embeddings em um espaço 2D usando t-SNE.\n",
    "    Args:\n",
    "        modelo: Modelo Word2Vec\n",
    "        palavras: Lista de palavras específicas para visualizar (opcional)\n",
    "        n_palavras: Número de palavras mais frequentes a visualizar (se palavras=None)\n",
    "    \"\"\"\n",
    "    # Obter palavras e vetores\n",
    "    if palavras is None:\n",
    "        palavras = list(modelo.wv.key_to_index.keys())[:n_palavras]\n",
    "    else:\n",
    "        # Filtrar palavras que não estão no vocabulário\n",
    "        palavras = [p for p in palavras if p in modelo.wv]\n",
    "    \n",
    "    if not palavras:\n",
    "        print(\"Nenhuma palavra válida para visualização\")\n",
    "        return\n",
    "    \n",
    "    # Obter vetores e converter para array NumPy\n",
    "    vetores = np.array([modelo.wv[palavra] for palavra in palavras])\n",
    "    \n",
    "    # Reduzir dimensionalidade com t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(palavras)-1))\n",
    "    vetores_2d = tsne.fit_transform(vetores)\n",
    "    \n",
    "    # Criar gráfico\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plotar pontos\n",
    "    x = vetores_2d[:, 0]\n",
    "    y = vetores_2d[:, 1]\n",
    "    plt.scatter(x, y, c='blue', alpha=0.7)\n",
    "    \n",
    "    # Adicionar rótulos (palavras)\n",
    "    for i, palavra in enumerate(palavras):\n",
    "        plt.annotate(\n",
    "            palavra,\n",
    "            xy=(x[i], y[i]),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    plt.title(\"Visualização t-SNE dos Word Embeddings\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados com TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n",
      "\n",
      "Resultados com Word Embeddings:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.33      0.50      0.40         3\n",
      "weighted avg       0.44      0.67      0.53         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Dados rotulados para exemplo\n",
    "textos_rotulados = textos  # Usando os mesmos textos de antes\n",
    "sentimentos = [1, 1, 0, 0, 1, 0, 1, 0, 1, 1]  # 1: positivo, 0: negativo\n",
    "\n",
    "# Função para gerar vetores de documento usando embeddings\n",
    "def texto_para_vetor(texto, modelo):\n",
    "    \"\"\"Converte um texto em um vetor médio dos embeddings de suas palavras\"\"\"\n",
    "    palavras = preprocessar_texto(texto)\n",
    "    # Filtrar palavras que estão no vocabulário do modelo\n",
    "    palavras_no_vocab = [p for p in palavras if p in modelo.wv]\n",
    "    if not palavras_no_vocab:\n",
    "        # Se nenhuma palavra estiver no vocabulário, retorna vetor de zeros\n",
    "        return np.zeros(modelo.vector_size)\n",
    "    # Calcular a média dos vetores das palavras\n",
    "    vetores = [modelo.wv[palavra] for palavra in palavras_no_vocab]\n",
    "    return np.mean(vetores, axis=0)\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    textos_rotulados, sentimentos, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Abordagem com TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "clf_tfidf = LogisticRegression(random_state=42)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# 2. Abordagem com Word Embeddings\n",
    "X_train_emb = np.array([texto_para_vetor(texto, model) for texto in X_train])\n",
    "X_test_emb = np.array([texto_para_vetor(texto, model) for texto in X_test])\n",
    "\n",
    "clf_emb = LogisticRegression(random_state=42)\n",
    "clf_emb.fit(X_train_emb, y_train)\n",
    "y_pred_emb = clf_emb.predict(X_test_emb)\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"\\nResultados com TF-IDF:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "print(\"\\nResultados com Word Embeddings:\")\n",
    "print(classification_report(y_test, y_pred_emb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
